"""
prompt.py — PROMPT → LLM → PARSER chain
---------------------------------------
Single-file module that defines:
- SYSTEM_PROMPT and USER_PROMPT_TEMPLATE
- build_prompt(user_input, odata_json, state_json)
- Control (Pydantic) schema
- parse_llm_output(text) → (bot_text, Control)
- BotControlParser (LangChain output parser)
- build_prompt_llm_parser_chain() → PROMPT | LLM | PARSER runnable

Drop into: src/app/core/prompt.py (adjust imports if your layout differs)
"""
from __future__ import annotations

import json
import re
from typing import Any, Dict, List, Literal, Tuple

from pydantic import BaseModel, Field, ValidationError

# If you're using LangChain in your project (as shown in your codebase):
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import BaseOutputParser

# Your existing LLM factory (keeps model/keys in settings)
from app.core.llm import get_llm

# ======================================================================
# SYSTEM PROMPT — controller rules for the model
# ======================================================================
SYSTEM_PROMPT = (
    """
You are an SAP business assistant for maintaining plant addresses (Telephone / Postal / Fax) in SAP S/4HANA.

STRICT OUTPUT FORMAT (always):
<bot>
[Natural, friendly business-language reply to the user. No technical field names.]
</bot>
<control>
{ "action": "ask" | "collect" | "confirm" | "execute" | "done" | "error",
  "service": "<service_name or empty>",
  "entity": "<entity_set or empty>",
  "method": "GET" | "PATCH" | "POST" | "",
  "key_field": "<string key value or empty>",
  "fields_needed": ["..."],
  "fields_collected": { "FieldName": "Value" },
  "payload": { "FieldName": "Value" },
  "message": "<short controller hint for the app>",
  "choices": ["choice 1", "choice 2"]
}
</control>

Rules:
- Speak only in simple business language using field DESCRIPTIONS from the metadata (never show technical names in <bot>).
- Conversation plan: 1) Offer address types (entity descriptions) as a numbered menu. 2) Collect REQUIRED key fields first (put missing keys at the start of fields_needed). 3) Then offer OPTIONAL (non-key) fields as a numbered menu with a “Done” option; loop until user chooses Done. 4) ALWAYS show a human-readable summary of the action and ALL field values provided by the user, then set action="confirm". 5) Only after the user confirms, set action="execute" and include: service, entity (EntitySet), method (PATCH for updates unless user asked create/read), key_field, and payload.
- fields_needed MUST list missing required keys first; after keys are collected, include optional fields you plan to ask next.
- payload MUST use technical field names from metadata, but do NOT show them in <bot>.
- If anything is unclear, set action="ask" with a clear question and include choices when helpful.
- If you cannot comply, set action="error" with a concise message.
- Output ONLY the two blocks (<bot> and <control>) and ensure the JSON inside <control> is STRICTLY valid JSON (no trailing commas, no comments).
"""
).strip()

# ======================================================================
# USER PROMPT — runtime context (user text + OData + state)
# ======================================================================
USER_PROMPT_TEMPLATE = (
    """
User said: "{user_input}"

OData Services JSON:
{odata_json}

Conversation State:
{state_json}

Follow rules and generate next assistant message.
"""
).strip()


# ======================================================================
# Helper: build the combined prompt string (optional use)
# ======================================================================

def build_prompt(user_input: str, odata_json: Dict[str, Any], state_json: Dict[str, Any]) -> str:
    user_block = USER_PROMPT_TEMPLATE.format(
        user_input=user_input,
        odata_json=json.dumps(odata_json, ensure_ascii=False, indent=2)
        if not isinstance(odata_json, str) else odata_json,
        state_json=json.dumps(state_json, ensure_ascii=False, indent=2)
        if not isinstance(state_json, str) else state_json,
    )
    return (SYSTEM_PROMPT + "\n\n" + user_block).strip()


# ======================================================================
# Control schema + strict parser for <bot> and <control>
# ======================================================================
Action = Literal["ask", "collect", "confirm", "execute", "done", "error"]
Method = Literal["GET", "PATCH", "POST", ""]


class Control(BaseModel):
    action: Action
    service: str
    entity: str
    method: Method
    key_field: str
    fields_needed: List[str]
    fields_collected: Dict[str, Any]
    payload: Dict[str, Any]
    message: str
    choices: List[str] = Field(default_factory=list)


BOT_RE = re.compile(r"<bot>\s*(.*?)\s*</bot>", re.DOTALL | re.IGNORECASE)
CTRL_RE = re.compile(r"<control>\s*(\{.*?\})\s*</control>", re.DOTALL | re.IGNORECASE)


def parse_llm_output(text: str) -> Tuple[str, Control]:
    """Extracts the two blocks and validates the control JSON."""
    bot_match = BOT_RE.search(text or "")
    ctrl_match = CTRL_RE.search(text or "")
    if not bot_match or not ctrl_match:
        raise ValueError("Missing <bot> or <control> blocks")

    bot_text = bot_match.group(1).strip()
    raw_json = ctrl_match.group(1)

    try:
        ctrl_obj = json.loads(raw_json)
        control = Control(**ctrl_obj)
    except (json.JSONDecodeError, ValidationError) as e:
        raise ValueError(f"Invalid control JSON: {e}") from e

    return bot_text, control


# ======================================================================
# LangChain PARSER and the PROMPT | LLM | PARSER chain builder
# ======================================================================
class BotControlParser(BaseOutputParser):
    """LangChain output parser that returns {"bot": str, "control": Control}."""

    def parse(self, text: str) -> Dict[str, Any]:
        bot, ctrl = parse_llm_output(text)
        return {"bot": bot, "control": ctrl}


def build_prompt_llm_parser_chain():
    """Returns a LangChain runnable: PROMPT | LLM | PARSER.
    Inputs: {user_input, odata_json, state_json}
    Output: {"bot": str, "control": Control}
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_PROMPT),
        ("human", USER_PROMPT_TEMPLATE),
    ])
    llm = get_llm()  # your existing factory
    parser = BotControlParser()
    return prompt | llm | parser
